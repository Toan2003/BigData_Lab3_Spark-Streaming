{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd75b7ca-d2dc-49f6-81ed-8c97a62f84ab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Command hỗ trợ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba58b256-7c9e-4ea7-ab3c-a3928f12513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe23f31-c78c-40d6-89c2-84ca8ef95096",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b4a1e7-65e0-493e-a4ba-b9135b1072f2",
   "metadata": {},
   "source": [
    "### Khai báo các biến cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54421816-daee-4f5d-a90a-51b266f16309",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType, IntegerType\n",
    "\n",
    "path = \"./taxi-data/\"\n",
    "\n",
    "# Define the input schema\n",
    "yellow_schema = StructType([\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"VendorID\", IntegerType(), True),\n",
    "    StructField(\"tpep_pickup_datetime\", TimestampType(), True),\n",
    "    StructField(\"tpep_dropoff_datetime\", TimestampType(), True),\n",
    "    StructField(\"passenger_count\", DoubleType(), True),\n",
    "    StructField(\"trip_distance\", DoubleType(), True),\n",
    "    StructField(\"pickup_longitude\", DoubleType(), True),\n",
    "    StructField(\"pickup_latitude\", DoubleType(), True),\n",
    "    StructField(\"RatecodeID\", DoubleType(), True),\n",
    "    StructField(\"store_and_fwd_flag\", StringType(), True),\n",
    "    StructField(\"dropoff_longitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_latitude\", DoubleType(), True),\n",
    "    StructField(\"payment_type\", DoubleType(), True),\n",
    "    StructField(\"fare_amount\", DoubleType(), True),\n",
    "    StructField(\"extra\", DoubleType(), True),\n",
    "    StructField(\"mta_tax\", DoubleType(), True),\n",
    "    StructField(\"tip_amount\", DoubleType(), True),\n",
    "    StructField(\"tolls_amount\", DoubleType(), True),\n",
    "    StructField(\"improvement_surcharge\", DoubleType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "green_schema = StructType([\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"VendorID\", IntegerType(), True),\n",
    "    StructField(\"lpep_pickup_datetime\", TimestampType(), True),\n",
    "    StructField(\"Lpep_dropoff_datetime\", TimestampType(), True),\n",
    "    StructField(\"Store_and_fwd_flag\", StringType(), True),\n",
    "    StructField(\"RateCodeID\", DoubleType(), True),\n",
    "    StructField(\"Pickup_longitude\", DoubleType(), True),\n",
    "    StructField(\"Pickup_latitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_longitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_latitude\", DoubleType(), True),\n",
    "    StructField(\"Passenger_count\", DoubleType(), True),\n",
    "    StructField(\"Trip_distance\", DoubleType(), True),\n",
    "    StructField(\"Fare_amount\", DoubleType(), True),\n",
    "    StructField(\"Extra\", DoubleType(), True),\n",
    "    StructField(\"MTA_tax\", DoubleType(), True),\n",
    "    StructField(\"Tip_amount\", DoubleType(), True),\n",
    "    StructField(\"Tolls_amount\", DoubleType(), True),\n",
    "    StructField(\"Ehail_fee\", DoubleType(), True),\n",
    "    StructField(\"improvement_surcharge\", DoubleType(), True),\n",
    "    StructField(\"Total_amount\", DoubleType(), True),\n",
    "    StructField(\"Payment_type\", DoubleType(), True),\n",
    "    StructField(\"Trip_type\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "default_schema = StructType([\n",
    "    StructField(\"_c0\", StringType(), True),\n",
    "    StructField(\"_c1\", StringType(), True),\n",
    "    StructField(\"_c2\", StringType(), True),\n",
    "    StructField(\"_c3\", StringType(), True),\n",
    "    StructField(\"_c4\", StringType(), True),\n",
    "    StructField(\"_c5\", StringType(), True),\n",
    "    StructField(\"_c6\", StringType(), True),\n",
    "    StructField(\"_c7\", StringType(), True),\n",
    "    StructField(\"_c8\", StringType(), True),\n",
    "    StructField(\"_c9\", StringType(), True),\n",
    "    StructField(\"_c10\", StringType(), True),\n",
    "    StructField(\"_c11\", StringType(), True),\n",
    "    StructField(\"_c12\", StringType(), True),\n",
    "    StructField(\"_c13\", StringType(), True),\n",
    "    StructField(\"_c14\", StringType(), True),\n",
    "    StructField(\"_c15\", StringType(), True),\n",
    "    StructField(\"_c16\", StringType(), True),\n",
    "    StructField(\"_c17\", StringType(), True),\n",
    "    StructField(\"_c18\", StringType(), True),\n",
    "    StructField(\"_c19\", StringType(), True),\n",
    "    StructField(\"_c20\", StringType(), True),\n",
    "    StructField(\"_c21\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "goldman = [(-74.0141012, 40.7152191), (-74.013777, 40.7152275), (-74.0141027, 40.7138745), (-74.0144185, 40.7140753)]\n",
    "# goldman = [(40.7152191, -74.0141012), (40.7152275, -74.013777), (40.7138745, -74.0141027), (40.7140753, -74.0144185)]\n",
    "citigroup = [(-74.011869, 40.7217236), (-74.009867, 40.721493), (-74.010140,40.720053), (-74.012083, 40.720267)]\n",
    "# citigroup = \"[(40.7217236, -74.011869), (40.721493, -74.009867), (40.720053, -74.01014), (40.720267, -74.012083)]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f27f4c51-8ae7-4527-8d9d-7e855e45aa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.functions import udf\n",
    "from shapely.geometry import Point, Polygon\n",
    "\n",
    "from shapely.geometry import Point, Polygon\n",
    "\n",
    "# Convert each coordinate pair to a Point object\n",
    "goldman_points = [Point(coords[0], coords[1]) for coords in goldman]\n",
    "citigroup_points = [Point(coords[0], coords[1]) for coords in citigroup]\n",
    "\n",
    "# Create the Polygon object using the list of points\n",
    "goldman_polygon = Polygon(goldman_points)\n",
    "citigroup_polygon = Polygon(citigroup_points)\n",
    "\n",
    "# Define a UDF to check if a point is inside the Goldman Sachs or Citigroup polygon\n",
    "def inside_polygon(longitude, latitude, polygon):\n",
    "    point = Point(longitude, latitude)\n",
    "    return point.within(polygon)\n",
    "\n",
    "# Adding a column to indicate the drop-off location\n",
    "def get_dropoff_location(longitude, latitude):\n",
    "    point = Point(longitude, latitude)\n",
    "    if point.within(goldman_polygon):\n",
    "        return \"Goldman Sachs\"\n",
    "    elif point.within(citigroup_polygon):\n",
    "        return \"Citigroup\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Register UDFs for both locations\n",
    "inside_goldman = udf(lambda lon, lat: inside_polygon(lon, lat, goldman_polygon), BooleanType())\n",
    "inside_citigroup = udf(lambda lon, lat: inside_polygon(lon, lat, citigroup_polygon), BooleanType())\n",
    "\n",
    "# Register UDFs for get_dropoff_location\n",
    "get_dropoff_location_udf = udf(get_dropoff_location, StringType())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea7fd68-1237-4142-8b65-90180740a87f",
   "metadata": {},
   "source": [
    "### Xử lý"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec846d33-a926-4a8b-ace2-000fc0b5c05c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/22 19:41:04 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-06727a3d-762c-4ab0-b2da-c6ac104c283a. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/05/22 19:41:04 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The number of arrivals to Citigroup has doubled from 3 to 12 at 3240000!'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'The number of arrivals to Citigroup has doubled from 3 to 10 at 5100000!'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, lit\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"TrendingArrivals\").getOrCreate()\n",
    "\n",
    "# Read the input stream using the default schema\n",
    "input_stream = spark.readStream \\\n",
    "    .schema(default_schema) \\\n",
    "    .csv(path)\n",
    "\n",
    "# Filter and apply schema for yellow taxi trips\n",
    "yellow_trips = input_stream.filter(col(\"_c0\") == \"yellow\") \\\n",
    "                           .drop(\"_c20\",\"_c21\") \\\n",
    "                           .toDF(*yellow_schema.names) \\\n",
    "\n",
    "# Cast each column based on yellow_schema data types\n",
    "for col_name, field in zip(yellow_schema.names, yellow_schema.fields):\n",
    "    yellow_trips = yellow_trips.withColumn(col_name, yellow_trips[col_name].cast(field.dataType))\n",
    "\n",
    "# yellow_trips.printSchema()\n",
    "\n",
    "# Filter and apply schema for green taxi trips\n",
    "green_trips = input_stream.filter(col(\"_c0\") == \"green\") \\\n",
    "                          .toDF(*green_schema.names)\n",
    "\n",
    "# Cast each column based on yellow_schema data types\n",
    "for col_name, field in zip(green_schema.names, green_schema.fields):\n",
    "    green_trips = green_trips.withColumn(col_name, green_trips[col_name].cast(field.dataType))\n",
    "\n",
    "# green_trips.printSchema()\n",
    "\n",
    "# Filter the stream for dropoff locations within the bounding boxes\n",
    "# yellow\n",
    "goldman_sachs_yellow_arrivals = yellow_trips \\\n",
    "  .filter(inside_goldman(col(\"dropoff_longitude\"), col(\"dropoff_latitude\"))) \\\n",
    "  .select(\"tpep_dropoff_datetime\") \\\n",
    "  .withColumnRenamed(\"tpep_dropoff_datetime\",\"dropoff_datetime\") \\\n",
    "  .withColumn(\"location\", lit(\"goldman\"))\n",
    "\n",
    "citigroup_yellow_arrivals = yellow_trips \\\n",
    "  .filter(inside_citigroup(col(\"dropoff_longitude\"), col(\"dropoff_latitude\"))) \\\n",
    "  .select(\"tpep_dropoff_datetime\") \\\n",
    "  .withColumnRenamed(\"tpep_dropoff_datetime\",\"dropoff_datetime\") \\\n",
    "  .withColumn(\"location\", lit(\"citigroup\"))\n",
    "\n",
    "# green\n",
    "goldman_sachs_green_arrivals = green_trips \\\n",
    "    .filter(inside_goldman(col(\"dropoff_longitude\"), col(\"dropoff_latitude\"))) \\\n",
    "    .select(\"Lpep_dropoff_datetime\") \\\n",
    "    .withColumnRenamed(\"Lpep_dropoff_datetime\",\"dropoff_datetime\") \\\n",
    "    .withColumn(\"location\", lit(\"goldman\"))\n",
    "\n",
    "citigroup_green_arrivals = green_trips \\\n",
    "    .filter(inside_citigroup(col(\"dropoff_longitude\"), col(\"dropoff_latitude\"))) \\\n",
    "    .select(\"Lpep_dropoff_datetime\") \\\n",
    "    .withColumnRenamed(\"Lpep_dropoff_datetime\",\"dropoff_datetime\") \\\n",
    "    .withColumn(\"location\", lit(\"citigroup\"))\n",
    "\n",
    "goldman_sachs_arrivals = goldman_sachs_yellow_arrivals.union(goldman_sachs_green_arrivals)\n",
    "citigroup_arrivals =citigroup_yellow_arrivals.union(citigroup_green_arrivals)\n",
    "\n",
    "# # Union the two streams\n",
    "arrivals = goldman_sachs_arrivals.union(citigroup_arrivals)\n",
    "\n",
    "# Define a 10-minute window and count arrivals\n",
    "ten_min_window = arrivals \\\n",
    "    .withWatermark(\"dropoff_datetime\", \"10 minutes\") \\\n",
    "    .groupBy(window(\"dropoff_datetime\", \"10 minutes\"), \"location\") \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed(\"count\", \"current_count\") \\\n",
    "    .withColumn(\"timestamp\",(unix_timestamp(col(\"window.end\")) - unix_timestamp(lit(\"2015-12-01 00:00:00\"))) * 100) \\\n",
    "    .orderBy(\"window\",\"location\")\n",
    "\n",
    "from IPython.display import display\n",
    "import os\n",
    "\n",
    "def foreach_batch_function(df, epoch_id):\n",
    "    goldman_df = df.filter(col(\"location\") == \"goldman\").rdd.map(tuple).collect()\n",
    "    citigroup_df = df.filter(col(\"location\") == \"citigroup\").rdd.map(tuple).collect()\n",
    "\n",
    "    path = f\"output/\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    for i in range(1,len(goldman_df)):\n",
    "        if goldman_df[i][2] >= 10:\n",
    "            if goldman_df[i][3] - 60000 == goldman_df[i-1][3]:\n",
    "                if goldman_df[i][2] >= goldman_df[i-1][2]*2:\n",
    "                    print(f\"The number of arrivals to Goldman Sachs has doubled from {goldman_df[i-1][2]} to {goldman_df[i][2]} at {goldman_df[i][3]}!\")\n",
    "                    display(f\"The number of arrivals to Goldman Sachs has doubled from {goldman_df[i-1][2]} to {goldman_df[i][2]} at {goldman_df[i][3]}!\")\n",
    "                    with open(os.path.join(path, f\"part-{goldman_df[i][3]}.txt\"), 'a') as file:\n",
    "                        file.write(f\"(goldman,({goldman_df[i][2]},{goldman_df[i][3]},{goldman_df[i-1][2]}))\\n\")\n",
    "            else:\n",
    "                display(f\"The number of arrivals to Goldman Sachs has doubled from 0 to {goldman_df[i][2]} at {goldman_df[i][3]}!\")\n",
    "                print(f\"The number of arrivals to Goldman Sachs has doubled from 0 to {goldman_df[i][2]} at {goldman_df[i][3]}!\")\n",
    "                with open(os.path.join(path, f\"part-{goldman_df[i][3]}.txt\"), 'a') as file:\n",
    "                        file.write(f\"(goldman,({goldman_df[i][2]},{goldman_df[i][3]},0))\\n\")\n",
    "\n",
    "    for i in range(1,len(citigroup_df)):\n",
    "        if citigroup_df[i][2] >= 10:\n",
    "            if citigroup_df[i][3] - 60000 == citigroup_df[i-1][3]:\n",
    "                if citigroup_df[i][2] >= citigroup_df[i-1][2]*2:\n",
    "                    print(f\"The number of arrivals to Citigroup has doubled from {citigroup_df[i-1][2]} to {citigroup_df[i][2]} at {citigroup_df[i][3]}!\")\n",
    "                    display(f\"The number of arrivals to Citigroup has doubled from {citigroup_df[i-1][2]} to {citigroup_df[i][2]} at {citigroup_df[i][3]}!\")\n",
    "                    with open(os.path.join(path, f\"part-{citigroup_df[i][3]}.txt\"), 'a') as file:\n",
    "                        file.write(f\"(citigroup,({citigroup_df[i][2]},{citigroup_df[i][3]},{citigroup_df[i-1][2]}))\\n\")\n",
    "            else:\n",
    "                print(f\"The number of arrivals to Citigroup has doubled from {citigroup_df[i-1][2]} to {citigroup_df[i][2]} at {citigroup_df[i][3]}!\")\n",
    "                display(f\"The number of arrivals to Citigroup has doubled from 0 to {citigroup_df[i][2]} at {citigroup_df[i][3]}!\")\n",
    "                with open(os.path.join(path, f\"part-{citigroup_df[i][3]}.txt\"), 'a') as file:\n",
    "                        file.write(f\"(citigroup,({citigroup_df[i][2]},{citigroup_df[i][3]},0))\\n\")\n",
    "\n",
    "# Calculate the previous count and detect trends\n",
    "# previous_count = ten_min_window \\\n",
    "#     .filter(\"current_count >= 10 \") \\\n",
    "#     .select(\"location\", \"current_count\", \"timestamp\", \"previous_count\")\n",
    "\n",
    "\n",
    "# previous_count = ten_min_window \\\n",
    "#     .join(withColumn(\"previous_count\", lag(\"current_count\", 1).over(window(\"location\", \"unboundedPreceding\", \"unboundedFollowing\"))), on=\"location\", how=\"left\") \\\n",
    "    # .drop(\"window\") \\\n",
    "    # .withColumnRenamed(\"lag(current_count, 1, null)\", \"previous_count\") \\\n",
    "    # .withColumnRenamed(\"window\", \"timestamp\") \\\n",
    "    # .filter((col(\"current_count\") >= 10) & (col(\"current_count\") >= 2 * col(\"previous_count\"))) \\\n",
    "    # .select(\"location\", \"current_count\", \"window.start\", \"previous_count\")\n",
    "\n",
    "# Output the trend detections to console\n",
    "# console_output = previous_count \\\n",
    "#     .select(when(col(\"location\") == \"goldman\", concat(lit(\"The number of arrivals to Goldman Sachs has doubled from \"), col(\"previous_count\"), lit(\" to \"), col(\"current_count\"), lit(\" at \"), col(\"timestamp\")))\n",
    "#              .otherwise(concat(lit(\"The number of arrivals to Citigroup has doubled from \"), col(\"previous_count\"), lit(\" to \"), col(\"current_count\"), lit(\" at \"), col(\"timestamp\")))\n",
    "#              .alias(\"message\"))\n",
    "\n",
    "# # Write the results to the output directory\n",
    "# # file_output = previous_count \\\n",
    "# #     .select(\"location\", struct(\"current_count\", \"timestamp\", \"previous_count\").alias(\"values\"))\n",
    "\n",
    "# console_query = console_output.writeStream \\\n",
    "#     .outputMode(\"complete\") \\\n",
    "#     .format(\"console\") \\\n",
    "#     .start()\n",
    "\n",
    "# file_query = file_output.writeStream \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .format(\"json\") \\\n",
    "#     .option(\"checkpointLocation\", \"checkpoint\") \\\n",
    "#     .option(\"path\", \"output\") \\\n",
    "#     .start()\n",
    "\n",
    "# console_query.awaitTermination()\n",
    "# file_query.awaitTermination()\n",
    "\n",
    "stream_query = ten_min_window.writeStream \\\n",
    "                         .format(\"console\") \\\n",
    "                         .outputMode(\"Complete\") \\\n",
    "                         .foreachBatch(foreach_batch_function) \\\n",
    "                         .start()\n",
    "# stream_query1 = ten_min_window.writeStream \\\n",
    "#                          .format(\"console\") \\\n",
    "#                          .outputMode(\"append\") \\\n",
    "#                          .start()\n",
    "\n",
    "stream_query.awaitTermination()\n",
    "# stream_query1.awaitTermination()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
