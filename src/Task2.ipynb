{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EventCount\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# set the path to the data\n",
    "data_path = \"taxi-data\"\n",
    "\n",
    "# Get active streaming queries\n",
    "active_queries = spark.streams.active\n",
    "\n",
    "# Stop each active streaming query\n",
    "for query in active_queries:\n",
    "    query.stop()\n",
    "\n",
    "# Define the directory path\n",
    "directory = \"output\"\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "if os.path.exists(directory):\n",
    "    shutil.rmtree(directory)\n",
    "os.makedirs(directory)\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"type\", StringType()),\n",
    "    StructField(\"VendorID\", IntegerType()),\n",
    "    StructField(\"pickup_datetime\", TimestampType()),\n",
    "    StructField(\"dropoff_datetime\", TimestampType()),\n",
    "    StructField(\"passenger_count\", IntegerType()),\n",
    "    StructField(\"trip_distance\", DoubleType()),\n",
    "    StructField(\"pickup_longitude\", DoubleType()),\n",
    "    StructField(\"pickup_latitude\", DoubleType()),\n",
    "    StructField(\"RatecodeID\", IntegerType()),\n",
    "    StructField(\"store_and_fwd_flag\", StringType()),\n",
    "    StructField(\"dropoff_longitude\", DoubleType()),\n",
    "    StructField(\"dropoff_latitude\", DoubleType()),\n",
    "    StructField(\"payment_type\", IntegerType()),\n",
    "    StructField(\"fare_amount\", DoubleType()),\n",
    "    StructField(\"extra\", DoubleType()),\n",
    "    StructField(\"mta_tax\", DoubleType()),\n",
    "    StructField(\"tip_amount\", DoubleType()),\n",
    "    StructField(\"tolls_amount\", DoubleType()),\n",
    "    StructField(\"improvement_surcharge\", DoubleType()),\n",
    "    StructField(\"total_amount\", DoubleType())\n",
    "])\n",
    "\n",
    "# Define the streaming DataFrame\n",
    "streaming_df = (\n",
    "    spark.readStream\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"false\")\n",
    "    .schema(schema)\n",
    "    .load(data_path)\n",
    ")\n",
    "\n",
    "# Initialize a dictionary to store running totals for each hour\n",
    "hour_counts = {}\n",
    "\n",
    "\n",
    "def process_streaming_data(df, epoch_id):\n",
    "    global hour_counts\n",
    "\n",
    "    # Calculate counts for each hour in the current batch\n",
    "    batch_hour_counts = df \\\n",
    "        .withColumn(\"hour\", f.hour(df[\"dropoff_datetime\"].cast(\"timestamp\"))) \\\n",
    "        .groupBy(\"hour\") \\\n",
    "        .count() \\\n",
    "        .collect()\n",
    "\n",
    "    # Update running totals with counts from the current batch\n",
    "    for row in batch_hour_counts:\n",
    "        hour = row[\"hour\"]\n",
    "        count = row[\"count\"]\n",
    "        if hour in hour_counts:\n",
    "            hour_counts[hour] += count\n",
    "        else:\n",
    "            hour_counts[hour] = count\n",
    "\n",
    "    # Write the updated counts to the corresponding output directories\n",
    "    for hour, count in hour_counts.items():\n",
    "        output_directory = f\"output/output-{(hour + 1) * 3600000}\"\n",
    "        row_df = spark.createDataFrame([(hour, count)], [\"hour\", \"count\"])\n",
    "        # Repartition the DataFrame before writing\n",
    "        row_df = row_df.repartition(1)\n",
    "        row_df.write.csv(output_directory, mode=\"overwrite\", header=\"true\")\n",
    "\n",
    "\n",
    "# Checkpoint location\n",
    "checkpoint_location = \"checkpoint\"\n",
    "\n",
    "# Start the new streaming query\n",
    "query = streaming_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .foreachBatch(process_streaming_data) \\\n",
    "    .option(\"checkpointLocation\", checkpoint_location)  # Add checkpoint location\n",
    "\n",
    "\n",
    "query.start().awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
