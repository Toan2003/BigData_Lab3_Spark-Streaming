{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58016f5c-de32-4106-9b70-e4d4f3b89a6a",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57ea9fe9-b6b8-4ac7-ac66-726235c8071b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/21 15:45:11 WARN Utils: Your hostname, DESKTOP-J1VUCON resolves to a loopback address: 127.0.1.1; using 172.30.215.130 instead (on interface eth0)\n",
      "24/05/21 15:45:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/21 15:45:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DoubleType, IntegerType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, window, when, unix_timestamp, format_string\n",
    "from pyspark.sql import functions as f\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "inputPath = \"file:///root/lab3/taxi-data\"\n",
    "outputPath = \"file:///root/lab3/output\"\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.master(\"local\")\\\n",
    "    .appName(\"Introduction to Spark\")\\\n",
    "    .config(\"spark.some.config.option\", \"some-value\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")\n",
    "spark.conf.set(\"spark.sql.streaming.statefulOperator.checkCorrectness.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38b6cd05-eb74-4232-ac1e-20e143e7f395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for yellow taxi rides\n",
    "schema_yellow = StructType() \\\n",
    "    .add(\"type\", StringType()) \\\n",
    "    .add(\"VendorID\", StringType()) \\\n",
    "    .add(\"tpep_pickup_datetime\", TimestampType()) \\\n",
    "    .add(\"tpep_dropoff_datetime\", TimestampType()) \\\n",
    "    .add(\"passenger_count\", StringType()) \\\n",
    "    .add(\"trip_distance\", StringType()) \\\n",
    "    .add(\"pickup_longitude\", DoubleType()) \\\n",
    "    .add(\"pickup_latitude\", DoubleType()) \\\n",
    "    .add(\"RatecodeID\", StringType()) \\\n",
    "    .add(\"store_and_fwd_flag\", StringType()) \\\n",
    "    .add(\"dropoff_longitude\", DoubleType()) \\\n",
    "    .add(\"dropoff_latitude\", DoubleType()) \\\n",
    "    .add(\"payment_type\", StringType()) \\\n",
    "    .add(\"fare_amount\", StringType()) \\\n",
    "    .add(\"extra\", StringType()) \\\n",
    "    .add(\"mta_tax\", StringType()) \\\n",
    "    .add(\"tip_amount\", StringType()) \\\n",
    "    .add(\"tolls_amount\", StringType()) \\\n",
    "    .add(\"improvement_surcharge\", StringType()) \\\n",
    "    .add(\"total_amount\", StringType())\n",
    "\n",
    "# Define the schema for green taxi rides\n",
    "schema_green = StructType() \\\n",
    "    .add(\"type\", StringType()) \\\n",
    "    .add(\"VendorID\", StringType()) \\\n",
    "    .add(\"lpep_pickup_datetime\", TimestampType()) \\\n",
    "    .add(\"lpep_dropoff_datetime\", TimestampType()) \\\n",
    "    .add(\"store_and_fwd_flag\", StringType()) \\\n",
    "    .add(\"RatecodeID\", StringType()) \\\n",
    "    .add(\"pickup_longitude\", DoubleType()) \\\n",
    "    .add(\"pickup_latitude\", DoubleType()) \\\n",
    "    .add(\"dropoff_longitude\", DoubleType()) \\\n",
    "    .add(\"dropoff_latitude\", DoubleType()) \\\n",
    "    .add(\"passenger_count\", StringType()) \\\n",
    "    .add(\"trip_distance\", StringType()) \\\n",
    "    .add(\"fare_amount\", StringType()) \\\n",
    "    .add(\"extra\", StringType()) \\\n",
    "    .add(\"mta_tax\", StringType()) \\\n",
    "    .add(\"tip_amount\", StringType()) \\\n",
    "    .add(\"tolls_amount\", StringType()) \\\n",
    "    .add(\"ehail_fee\", StringType()) \\\n",
    "    .add(\"improvement_surcharge\", StringType()) \\\n",
    "    .add(\"total_amount\", StringType()) \\\n",
    "    .add(\"payment_type\", StringType()) \\\n",
    "    .add(\"trip_type\", StringType())\n",
    "\n",
    "default_schema = StructType([\n",
    "    StructField(\"_c0\", StringType(), True),\n",
    "    StructField(\"_c1\", StringType(), True),\n",
    "    StructField(\"_c2\", StringType(), True),\n",
    "    StructField(\"_c3\", StringType(), True),\n",
    "    StructField(\"_c4\", StringType(), True),\n",
    "    StructField(\"_c5\", StringType(), True),\n",
    "    StructField(\"_c6\", StringType(), True),\n",
    "    StructField(\"_c7\", StringType(), True),\n",
    "    StructField(\"_c8\", StringType(), True),\n",
    "    StructField(\"_c9\", StringType(), True),\n",
    "    StructField(\"_c10\", StringType(), True),\n",
    "    StructField(\"_c11\", StringType(), True),\n",
    "    StructField(\"_c12\", StringType(), True),\n",
    "    StructField(\"_c13\", StringType(), True),\n",
    "    StructField(\"_c14\", StringType(), True),\n",
    "    StructField(\"_c15\", StringType(), True),\n",
    "    StructField(\"_c16\", StringType(), True),\n",
    "    StructField(\"_c17\", StringType(), True),\n",
    "    StructField(\"_c18\", StringType(), True),\n",
    "    StructField(\"_c19\", StringType(), True),\n",
    "    StructField(\"_c20\", StringType(), True),\n",
    "    StructField(\"_c21\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "528c5b1a-f832-4b89-8ce3-c529755660bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_df = (\n",
    "    spark.readStream\n",
    "    .format(\"csv\")\n",
    "    .schema(default_schema)\n",
    "    .option(\"header\", \"false\")\n",
    "    .load(inputPath)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "605bbf88-3332-473a-8918-4f57479afbd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- RatecodeID: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter and apply schema for yellow taxi trips\n",
    "yellow_trips = default_df.filter(col(\"_c0\") == \"yellow\") \\\n",
    "                           .drop(\"_c20\",\"_c21\") \\\n",
    "                           .toDF(*schema_yellow.names)\n",
    "\n",
    "# Convert the columns as per the new schema\n",
    "yellow_df = yellow_trips.withColumn(\"tpep_pickup_datetime\", col(\"tpep_pickup_datetime\").cast(TimestampType())) \\\n",
    "    .withColumn(\"tpep_dropoff_datetime\", col(\"tpep_dropoff_datetime\").cast(TimestampType())) \\\n",
    "    .withColumn(\"pickup_longitude\", col(\"pickup_longitude\").cast(DoubleType())) \\\n",
    "    .withColumn(\"pickup_latitude\", col(\"pickup_latitude\").cast(DoubleType())) \\\n",
    "    .withColumn(\"dropoff_longitude\", col(\"dropoff_longitude\").cast(DoubleType())) \\\n",
    "    .withColumn(\"dropoff_latitude\", col(\"dropoff_latitude\").cast(DoubleType()))\n",
    "\n",
    "yellow_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16b44092-9bdb-478d-b93a-096a36af2713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- lpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- lpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- RatecodeID: string (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- ehail_fee: string (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- trip_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter and apply schema for green taxi trips\n",
    "green_trips = default_df.filter(col(\"_c0\") == \"green\") \\\n",
    "                           .toDF(*schema_green.names)\n",
    "\n",
    "# Convert the columns as per the new schema\n",
    "green_df = green_trips.withColumn(\"lpep_pickup_datetime\", col(\"lpep_pickup_datetime\").cast(TimestampType())) \\\n",
    "    .withColumn(\"lpep_dropoff_datetime\", col(\"lpep_dropoff_datetime\").cast(TimestampType())) \\\n",
    "    .withColumn(\"pickup_longitude\", col(\"pickup_longitude\").cast(DoubleType())) \\\n",
    "    .withColumn(\"pickup_latitude\", col(\"pickup_latitude\").cast(DoubleType())) \\\n",
    "    .withColumn(\"dropoff_longitude\", col(\"dropoff_longitude\").cast(DoubleType())) \\\n",
    "    .withColumn(\"dropoff_latitude\", col(\"dropoff_latitude\").cast(DoubleType()))\n",
    "\n",
    "green_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97866c5b-fb0b-4cc4-827e-09c58c2e941d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounding box coordinates for Goldman Sachs and Citigroup headquarters\n",
    "goldman_coords = [\n",
    "    (-74.0141012, 40.7152191),\n",
    "    (-74.013777, 40.7152275),\n",
    "    (-74.0141027, 40.7138745),\n",
    "    (-74.0144185, 40.7140753)\n",
    "]\n",
    "\n",
    "citigroup_coords = [\n",
    "    (-74.011869, 40.7217236),\n",
    "    (-74.009867, 40.721493),\n",
    "    (-74.010140, 40.720053),\n",
    "    (-74.012083, 40.720267)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b101507-9dbe-4ca8-942d-121b37113bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from shapely.geometry import Point, Polygon\n",
    "from pyspark.sql.types import BooleanType\n",
    "\n",
    "# Create polygons from coordinates\n",
    "goldman_polygon = Polygon(goldman_coords)\n",
    "citigroup_polygon = Polygon(citigroup_coords)\n",
    "\n",
    "# Define a UDF to check if a point is inside the Goldman Sachs or Citigroup polygon\n",
    "def inside_polygon(longitude, latitude, polygon):\n",
    "    point = Point(longitude, latitude)\n",
    "    return point.within(polygon)\n",
    "\n",
    "# Register UDFs for both locations\n",
    "inside_goldman = udf(lambda lon, lat: inside_polygon(lon, lat, goldman_polygon), BooleanType())\n",
    "inside_citigroup = udf(lambda lon, lat: inside_polygon(lon, lat, citigroup_polygon), BooleanType())\n",
    "\n",
    "# Adding a column to indicate the drop-off location\n",
    "def get_dropoff_location(longitude, latitude):\n",
    "    point = Point(longitude, latitude)\n",
    "    if point.within(goldman_polygon):\n",
    "        return \"Goldman Sachs\"\n",
    "    elif point.within(citigroup_polygon):\n",
    "        return \"Citigroup\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "get_dropoff_location_udf = udf(get_dropoff_location, StringType())\n",
    "\n",
    "# Filter dataframe for trips that end within the specified bounding boxes\n",
    "yellow_trips = yellow_df.filter(\n",
    "    inside_goldman(col(\"dropoff_longitude\"), col(\"dropoff_latitude\")) | \n",
    "    inside_citigroup(col(\"dropoff_longitude\"), col(\"dropoff_latitude\"))\n",
    ")\n",
    "\n",
    "yellow_trips = yellow_trips.withColumn(\"dropoff_location\", get_dropoff_location_udf(col(\"dropoff_longitude\"), col(\"dropoff_latitude\")))\n",
    "\n",
    "green_trips = green_df.filter(\n",
    "    inside_goldman(col(\"dropoff_longitude\"), col(\"dropoff_latitude\")) | \n",
    "    inside_citigroup(col(\"dropoff_longitude\"), col(\"dropoff_latitude\"))\n",
    ")\n",
    "\n",
    "green_trips = green_trips.withColumn(\"dropoff_location\", get_dropoff_location_udf(col(\"dropoff_longitude\"), col(\"dropoff_latitude\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d65f1cf-6190-4c3a-ace3-ea0c2c8b49c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming dropoff_datetime is the column based on which windowed aggregation is done\n",
    "yellow_agg_df = (\n",
    "    yellow_trips\n",
    "        .groupBy(\n",
    "            yellow_trips.dropoff_location,\n",
    "            window(col(\"tpep_dropoff_datetime\"), \"1 hour\"),\n",
    "        )\n",
    "        .count()\n",
    "        .orderBy(\"window\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61223870-6896-43cc-bca5-383c48171dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming dropoff_datetime is the column based on which windowed aggregation is done\n",
    "green_agg_df = (\n",
    "    green_trips\n",
    "        .groupBy(\n",
    "            green_trips.dropoff_location,\n",
    "            window(col(\"lpep_dropoff_datetime\"), \"1 hour\"),\n",
    "        )\n",
    "        .count()\n",
    "        .orderBy(\"window\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb0ec8ac-d550-4805-ac59-64877e320213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast the 'count' column to integer if necessary\n",
    "yellow_agg_df = yellow_agg_df.withColumn(\"count\", when(col(\"count\").cast(\"integer\").isNull(), 0).otherwise(col(\"count\").cast(\"integer\")))\n",
    "green_agg_df = green_agg_df.withColumn(\"count\", when(col(\"count\").cast(\"integer\").isNull(), 0).otherwise(col(\"count\").cast(\"integer\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcc35f61-5e9a-424f-818c-19d0bdbde93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = yellow_agg_df.union(green_agg_df).orderBy(\"window\")\n",
    "final_agg_df = combined_df.groupBy(\"dropoff_location\", \"window\").sum(\"count\")\n",
    "final_agg_df = final_agg_df.withColumnRenamed(\"sum(count)\", \"count\")\n",
    "\n",
    "# Format the path string to include the timestamp as milliseconds\n",
    "final_agg_df = final_agg_df.withColumn(\"path\", format_string(\"output-%d\", (f.hour(col(\"window.start\")) + 1) * 360000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa14099c-bb62-4a5c-828e-a63f0ac3c81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_files(batch_df, epoch_id):\n",
    "    def write_partition(partition):\n",
    "        import os\n",
    "        for row in partition:\n",
    "            path = f\"output/{row['path']}\"\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            with open(os.path.join(path, f\"part-{epoch_id}.txt\"), 'a') as file:\n",
    "                file.write(f\"({row['dropoff_location']}, {row['count']})\\n\")\n",
    "\n",
    "    batch_df.foreachPartition(write_partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5281c1b2-609b-4205-87f7-cf55a28386d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/20 22:47:06 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/05/20 22:47:06 WARN UnsupportedOperationChecker: Detected pattern of possible 'correctness' issue due to global watermark. The query contains stateful operation which can emit rows older than the current watermark plus allowed late record delay, which are \"late rows\" in downstream stateful operations and these rows can be discarded. Please refer the programming guide doc for more details. If you understand the possible risk of correctness issue and still need to run the query, you can disable this check by setting the config `spark.sql.streaming.statefulOperator.checkCorrectness.enabled` to false.;\n",
      "Project [dropoff_location#416, window#465, count#542L, format_string(output-%d, ((hour(window#465.start, Some(Asia/Bangkok)) + 1) * 360000)) AS path#546]\n",
      "+- Project [dropoff_location#416, window#465, sum(count)#538L AS count#542L]\n",
      "   +- Aggregate [dropoff_location#416, window#465], [dropoff_location#416, window#465, sum(count#523) AS sum(count)#538L]\n",
      "      +- Sort [window#465 ASC NULLS FIRST], true\n",
      "         +- Union false, false\n",
      "            :- Project [dropoff_location#416, window#465, CASE WHEN isnull(cast(count#488L as int)) THEN 0 ELSE cast(count#488L as int) END AS count#523]\n",
      "            :  +- Sort [window#465 ASC NULLS FIRST], true\n",
      "            :     +- Aggregate [dropoff_location#416, window#489], [dropoff_location#416, window#489 AS window#465, count(1) AS count#488L]\n",
      "            :        +- Project [named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(tpep_dropoff_datetime#126, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(tpep_dropoff_datetime#126, TimestampType, LongType) - 0) % 3600000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(tpep_dropoff_datetime#126, TimestampType, LongType) - 0) % 3600000000) + 3600000000) ELSE ((precisetimestampconversion(tpep_dropoff_datetime#126, TimestampType, LongType) - 0) % 3600000000) END) - 0), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(tpep_dropoff_datetime#126, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(tpep_dropoff_datetime#126, TimestampType, LongType) - 0) % 3600000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(tpep_dropoff_datetime#126, TimestampType, LongType) - 0) % 3600000000) + 3600000000) ELSE ((precisetimestampconversion(tpep_dropoff_datetime#126, TimestampType, LongType) - 0) % 3600000000) END) - 0) + 3600000000), LongType, TimestampType))) AS window#489, type#65, VendorID#66, tpep_pickup_datetime#105, tpep_dropoff_datetime#126, passenger_count#69, trip_distance#70, pickup_longitude#147, pickup_latitude#168, RatecodeID#73, store_and_fwd_flag#74, dropoff_longitude#189, dropoff_latitude#210, payment_type#77, fare_amount#78, extra#79, mta_tax#80, tip_amount#81, tolls_amount#82, improvement_surcharge#83, total_amount#84, dropoff_location#416]\n",
      "            :           +- Filter isnotnull(tpep_dropoff_datetime#126)\n",
      "            :              +- Project [type#65, VendorID#66, tpep_pickup_datetime#105, tpep_dropoff_datetime#126, passenger_count#69, trip_distance#70, pickup_longitude#147, pickup_latitude#168, RatecodeID#73, store_and_fwd_flag#74, dropoff_longitude#189, dropoff_latitude#210, payment_type#77, fare_amount#78, extra#79, mta_tax#80, tip_amount#81, tolls_amount#82, improvement_surcharge#83, total_amount#84, get_dropoff_location(dropoff_longitude#189, dropoff_latitude#210)#415 AS dropoff_location#416]\n",
      "            :                 +- Filter (<lambda>(dropoff_longitude#189, dropoff_latitude#210)#413 OR <lambda>(dropoff_longitude#189, dropoff_latitude#210)#414)\n",
      "            :                    +- Project [type#65, VendorID#66, tpep_pickup_datetime#105, tpep_dropoff_datetime#126, passenger_count#69, trip_distance#70, pickup_longitude#147, pickup_latitude#168, RatecodeID#73, store_and_fwd_flag#74, dropoff_longitude#189, cast(dropoff_latitude#76 as double) AS dropoff_latitude#210, payment_type#77, fare_amount#78, extra#79, mta_tax#80, tip_amount#81, tolls_amount#82, improvement_surcharge#83, total_amount#84]\n",
      "            :                       +- Project [type#65, VendorID#66, tpep_pickup_datetime#105, tpep_dropoff_datetime#126, passenger_count#69, trip_distance#70, pickup_longitude#147, pickup_latitude#168, RatecodeID#73, store_and_fwd_flag#74, cast(dropoff_longitude#75 as double) AS dropoff_longitude#189, dropoff_latitude#76, payment_type#77, fare_amount#78, extra#79, mta_tax#80, tip_amount#81, tolls_amount#82, improvement_surcharge#83, total_amount#84]\n",
      "            :                          +- Project [type#65, VendorID#66, tpep_pickup_datetime#105, tpep_dropoff_datetime#126, passenger_count#69, trip_distance#70, pickup_longitude#147, cast(pickup_latitude#72 as double) AS pickup_latitude#168, RatecodeID#73, store_and_fwd_flag#74, dropoff_longitude#75, dropoff_latitude#76, payment_type#77, fare_amount#78, extra#79, mta_tax#80, tip_amount#81, tolls_amount#82, improvement_surcharge#83, total_amount#84]\n",
      "            :                             +- Project [type#65, VendorID#66, tpep_pickup_datetime#105, tpep_dropoff_datetime#126, passenger_count#69, trip_distance#70, cast(pickup_longitude#71 as double) AS pickup_longitude#147, pickup_latitude#72, RatecodeID#73, store_and_fwd_flag#74, dropoff_longitude#75, dropoff_latitude#76, payment_type#77, fare_amount#78, extra#79, mta_tax#80, tip_amount#81, tolls_amount#82, improvement_surcharge#83, total_amount#84]\n",
      "            :                                +- Project [type#65, VendorID#66, tpep_pickup_datetime#105, cast(tpep_dropoff_datetime#68 as timestamp) AS tpep_dropoff_datetime#126, passenger_count#69, trip_distance#70, pickup_longitude#71, pickup_latitude#72, RatecodeID#73, store_and_fwd_flag#74, dropoff_longitude#75, dropoff_latitude#76, payment_type#77, fare_amount#78, extra#79, mta_tax#80, tip_amount#81, tolls_amount#82, improvement_surcharge#83, total_amount#84]\n",
      "            :                                   +- Project [type#65, VendorID#66, cast(tpep_pickup_datetime#67 as timestamp) AS tpep_pickup_datetime#105, tpep_dropoff_datetime#68, passenger_count#69, trip_distance#70, pickup_longitude#71, pickup_latitude#72, RatecodeID#73, store_and_fwd_flag#74, dropoff_longitude#75, dropoff_latitude#76, payment_type#77, fare_amount#78, extra#79, mta_tax#80, tip_amount#81, tolls_amount#82, improvement_surcharge#83, total_amount#84]\n",
      "            :                                      +- Project [_c0#0 AS type#65, _c1#1 AS VendorID#66, _c2#2 AS tpep_pickup_datetime#67, _c3#3 AS tpep_dropoff_datetime#68, _c4#4 AS passenger_count#69, _c5#5 AS trip_distance#70, _c6#6 AS pickup_longitude#71, _c7#7 AS pickup_latitude#72, _c8#8 AS RatecodeID#73, _c9#9 AS store_and_fwd_flag#74, _c10#10 AS dropoff_longitude#75, _c11#11 AS dropoff_latitude#76, _c12#12 AS payment_type#77, _c13#13 AS fare_amount#78, _c14#14 AS extra#79, _c15#15 AS mta_tax#80, _c16#16 AS tip_amount#81, _c17#17 AS tolls_amount#82, _c18#18 AS improvement_surcharge#83, _c19#19 AS total_amount#84]\n",
      "            :                                         +- Project [_c0#0, _c1#1, _c2#2, _c3#3, _c4#4, _c5#5, _c6#6, _c7#7, _c8#8, _c9#9, _c10#10, _c11#11, _c12#12, _c13#13, _c14#14, _c15#15, _c16#16, _c17#17, _c18#18, _c19#19]\n",
      "            :                                            +- Filter (_c0#0 = yellow)\n",
      "            :                                               +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@42d9ea48,csv,List(),Some(StructType(StructField(_c0,StringType,true),StructField(_c1,StringType,true),StructField(_c2,StringType,true),StructField(_c3,StringType,true),StructField(_c4,StringType,true),StructField(_c5,StringType,true),StructField(_c6,StringType,true),StructField(_c7,StringType,true),StructField(_c8,StringType,true),StructField(_c9,StringType,true),StructField(_c10,StringType,true),StructField(_c11,StringType,true),StructField(_c12,StringType,true),StructField(_c13,StringType,true),StructField(_c14,StringType,true),StructField(_c15,StringType,true),StructField(_c16,StringType,true),StructField(_c17,StringType,true),StructField(_c18,StringType,true),StructField(_c19,StringType,true),StructField(_c20,StringType,true),StructField(_c21,StringType,true))),List(),None,Map(header -> false, path -> file:///root/lab3/taxi-data),None), FileSource[file:///root/lab3/taxi-data], [_c0#0, _c1#1, _c2#2, _c3#3, _c4#4, _c5#5, _c6#6, _c7#7, _c8#8, _c9#9, _c10#10, _c11#11, _c12#12, _c13#13, _c14#14, _c15#15, _c16#16, _c17#17, _c18#18, _c19#19, _c20#20, _c21#21]\n",
      "            +- Project [dropoff_location#441, window#493, CASE WHEN isnull(cast(count#518L as int)) THEN 0 ELSE cast(count#518L as int) END AS count#527]\n",
      "               +- Sort [window#493 ASC NULLS FIRST], true\n",
      "                  +- Aggregate [dropoff_location#441, window#519], [dropoff_location#441, window#519 AS window#493, count(1) AS count#518L]\n",
      "                     +- Project [named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(lpep_dropoff_datetime#298, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(lpep_dropoff_datetime#298, TimestampType, LongType) - 0) % 3600000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(lpep_dropoff_datetime#298, TimestampType, LongType) - 0) % 3600000000) + 3600000000) ELSE ((precisetimestampconversion(lpep_dropoff_datetime#298, TimestampType, LongType) - 0) % 3600000000) END) - 0), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(lpep_dropoff_datetime#298, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(lpep_dropoff_datetime#298, TimestampType, LongType) - 0) % 3600000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(lpep_dropoff_datetime#298, TimestampType, LongType) - 0) % 3600000000) + 3600000000) ELSE ((precisetimestampconversion(lpep_dropoff_datetime#298, TimestampType, LongType) - 0) % 3600000000) END) - 0) + 3600000000), LongType, TimestampType))) AS window#519, type#231, VendorID#232, lpep_pickup_datetime#275, lpep_dropoff_datetime#298, store_and_fwd_flag#235, RatecodeID#236, pickup_longitude#321, pickup_latitude#344, dropoff_longitude#367, dropoff_latitude#390, passenger_count#241, trip_distance#242, fare_amount#243, extra#244, mta_tax#245, tip_amount#246, tolls_amount#247, ehail_fee#248, improvement_surcharge#249, total_amount#250, payment_type#251, trip_type#252, dropoff_location#441]\n",
      "                        +- Filter isnotnull(lpep_dropoff_datetime#298)\n",
      "                           +- Project [type#231, VendorID#232, lpep_pickup_datetime#275, lpep_dropoff_datetime#298, store_and_fwd_flag#235, RatecodeID#236, pickup_longitude#321, pickup_latitude#344, dropoff_longitude#367, dropoff_latitude#390, passenger_count#241, trip_distance#242, fare_amount#243, extra#244, mta_tax#245, tip_amount#246, tolls_amount#247, ehail_fee#248, improvement_surcharge#249, total_amount#250, payment_type#251, trip_type#252, get_dropoff_location(dropoff_longitude#367, dropoff_latitude#390)#440 AS dropoff_location#441]\n",
      "                              +- Filter (<lambda>(dropoff_longitude#367, dropoff_latitude#390)#438 OR <lambda>(dropoff_longitude#367, dropoff_latitude#390)#439)\n",
      "                                 +- Project [type#231, VendorID#232, lpep_pickup_datetime#275, lpep_dropoff_datetime#298, store_and_fwd_flag#235, RatecodeID#236, pickup_longitude#321, pickup_latitude#344, dropoff_longitude#367, cast(dropoff_latitude#240 as double) AS dropoff_latitude#390, passenger_count#241, trip_distance#242, fare_amount#243, extra#244, mta_tax#245, tip_amount#246, tolls_amount#247, ehail_fee#248, improvement_surcharge#249, total_amount#250, payment_type#251, trip_type#252]\n",
      "                                    +- Project [type#231, VendorID#232, lpep_pickup_datetime#275, lpep_dropoff_datetime#298, store_and_fwd_flag#235, RatecodeID#236, pickup_longitude#321, pickup_latitude#344, cast(dropoff_longitude#239 as double) AS dropoff_longitude#367, dropoff_latitude#240, passenger_count#241, trip_distance#242, fare_amount#243, extra#244, mta_tax#245, tip_amount#246, tolls_amount#247, ehail_fee#248, improvement_surcharge#249, total_amount#250, payment_type#251, trip_type#252]\n",
      "                                       +- Project [type#231, VendorID#232, lpep_pickup_datetime#275, lpep_dropoff_datetime#298, store_and_fwd_flag#235, RatecodeID#236, pickup_longitude#321, cast(pickup_latitude#238 as double) AS pickup_latitude#344, dropoff_longitude#239, dropoff_latitude#240, passenger_count#241, trip_distance#242, fare_amount#243, extra#244, mta_tax#245, tip_amount#246, tolls_amount#247, ehail_fee#248, improvement_surcharge#249, total_amount#250, payment_type#251, trip_type#252]\n",
      "                                          +- Project [type#231, VendorID#232, lpep_pickup_datetime#275, lpep_dropoff_datetime#298, store_and_fwd_flag#235, RatecodeID#236, cast(pickup_longitude#237 as double) AS pickup_longitude#321, pickup_latitude#238, dropoff_longitude#239, dropoff_latitude#240, passenger_count#241, trip_distance#242, fare_amount#243, extra#244, mta_tax#245, tip_amount#246, tolls_amount#247, ehail_fee#248, improvement_surcharge#249, total_amount#250, payment_type#251, trip_type#252]\n",
      "                                             +- Project [type#231, VendorID#232, lpep_pickup_datetime#275, cast(lpep_dropoff_datetime#234 as timestamp) AS lpep_dropoff_datetime#298, store_and_fwd_flag#235, RatecodeID#236, pickup_longitude#237, pickup_latitude#238, dropoff_longitude#239, dropoff_latitude#240, passenger_count#241, trip_distance#242, fare_amount#243, extra#244, mta_tax#245, tip_amount#246, tolls_amount#247, ehail_fee#248, improvement_surcharge#249, total_amount#250, payment_type#251, trip_type#252]\n",
      "                                                +- Project [type#231, VendorID#232, cast(lpep_pickup_datetime#233 as timestamp) AS lpep_pickup_datetime#275, lpep_dropoff_datetime#234, store_and_fwd_flag#235, RatecodeID#236, pickup_longitude#237, pickup_latitude#238, dropoff_longitude#239, dropoff_latitude#240, passenger_count#241, trip_distance#242, fare_amount#243, extra#244, mta_tax#245, tip_amount#246, tolls_amount#247, ehail_fee#248, improvement_surcharge#249, total_amount#250, payment_type#251, trip_type#252]\n",
      "                                                   +- Project [_c0#0 AS type#231, _c1#1 AS VendorID#232, _c2#2 AS lpep_pickup_datetime#233, _c3#3 AS lpep_dropoff_datetime#234, _c4#4 AS store_and_fwd_flag#235, _c5#5 AS RatecodeID#236, _c6#6 AS pickup_longitude#237, _c7#7 AS pickup_latitude#238, _c8#8 AS dropoff_longitude#239, _c9#9 AS dropoff_latitude#240, _c10#10 AS passenger_count#241, _c11#11 AS trip_distance#242, _c12#12 AS fare_amount#243, _c13#13 AS extra#244, _c14#14 AS mta_tax#245, _c15#15 AS tip_amount#246, _c16#16 AS tolls_amount#247, _c17#17 AS ehail_fee#248, _c18#18 AS improvement_surcharge#249, _c19#19 AS total_amount#250, _c20#20 AS payment_type#251, _c21#21 AS trip_type#252]\n",
      "                                                      +- Filter (_c0#0 = green)\n",
      "                                                         +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@42d9ea48,csv,List(),Some(StructType(StructField(_c0,StringType,true),StructField(_c1,StringType,true),StructField(_c2,StringType,true),StructField(_c3,StringType,true),StructField(_c4,StringType,true),StructField(_c5,StringType,true),StructField(_c6,StringType,true),StructField(_c7,StringType,true),StructField(_c8,StringType,true),StructField(_c9,StringType,true),StructField(_c10,StringType,true),StructField(_c11,StringType,true),StructField(_c12,StringType,true),StructField(_c13,StringType,true),StructField(_c14,StringType,true),StructField(_c15,StringType,true),StructField(_c16,StringType,true),StructField(_c17,StringType,true),StructField(_c18,StringType,true),StructField(_c19,StringType,true),StructField(_c20,StringType,true),StructField(_c21,StringType,true))),List(),None,Map(header -> false, path -> file:///root/lab3/taxi-data),None), FileSource[file:///root/lab3/taxi-data], [_c0#0, _c1#1, _c2#2, _c3#3, _c4#4, _c5#5, _c6#6, _c7#7, _c8#8, _c9#9, _c10#10, _c11#11, _c12#12, _c13#13, _c14#14, _c15#15, _c16#16, _c17#17, _c18#18, _c19#19, _c20#20, _c21#21]\n",
      "\n",
      "ERROR:root:KeyboardInterrupt while sending command.                             \n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7fcbaf49c940>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m final_agg_df\u001b[38;5;241m.\u001b[39mwriteStream\\\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39mforeachBatch(write_to_files)\\\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpointLocation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile:///root/checkpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m----> 7\u001b[0m \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/streaming/query.py:221\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "query = final_agg_df.writeStream\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .foreachBatch(write_to_files)\\\n",
    "    .option(\"checkpointLocation\", \"file:///root/checkpoint\")\\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e49179b-78ec-4724-8f57-a334d1f57835",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
